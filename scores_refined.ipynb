{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9ca8e-1d6d-4038-a5e3-c86dcd1449db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics[multimodal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549febe-48f7-41ec-9f6a-2aeb424fb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyiqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9745cdf1-e0a8-4921-9f47-3a6a405e20c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishore120/kishore_diffusion_env/lib/python3.8/site-packages/tqdm-4.66.2-py3.8.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "import torchmetrics.multimodal\n",
    "from torchmetrics.multimodal import CLIPImageQualityAssessment as CLIPIQA\n",
    "import pyiqa\n",
    "import  glob\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import lpips\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.restoration import estimate_sigma\n",
    "from torchvision import transforms\n",
    "#from clipiqa import CLIPIQA\n",
    "#from musiq import MUSIQ\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9db8e5-3935-471f-ac29-0aa7438f0d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model MUSIQ from /home/kishore120/.cache/torch/hub/pyiqa/musiq_koniq_ckpt-e95806b9.pth\n"
     ]
    }
   ],
   "source": [
    "musiq_metric = pyiqa.create_metric('musiq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74ab8b2-28af-4dac-a053-2a4f119d9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipiqa_metric = pyiqa.create_metric('clipiqa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05023d-6e67-4963-8c5e-b8083848d896",
   "metadata": {},
   "source": [
    "## Clip IQA score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed59d213-d885-4e5d-986d-c6f7855b755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saving path  /srv/Data16tb/Kishore/ResShift/results/user_own_data_prosx_t2_inference_SR4_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "# Set image folder path\n",
    "import os\n",
    "from PIL import Image\n",
    "output_folder = '/srv/Data16tb/Kishore/ResShift/results/user_own_data_prosx_t2_inference_SR4'\n",
    "# '/srv/Data16tb/Kishore/ResShift/results/Image_net_test_v1_64_64_32'\n",
    "# \"/srv/Data16tb/Kishore/ResShift/results/table6_journal_blid_face_restoration_faceIR\"\n",
    "save_path = os.path.join('/srv/Data16tb/Kishore/ResShift/results/',output_folder.split('/')[-1]+'_metrics.txt')\n",
    "print('Metrics saving path ',save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4539da-a87d-48a1-a79f-1012aba9ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images to process 1380\n",
      "Pred img shape (1024, 1024)\n",
      "\n",
      "Average CLIPIQA Score: 0.4967\n"
     ]
    }
   ],
   "source": [
    "clipiqa_scores = []\n",
    "\n",
    "# Get the list of images in the folder (filter only valid image files)\n",
    "output_images = [f for f in sorted(os.listdir(output_folder)) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "print(\"total images to process\",len(output_images))\n",
    "\n",
    "# # Define image transformation to convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Loop through images\n",
    "for n,img_name in enumerate(output_images):\n",
    "    # print(n,'---->>',img_name)\n",
    "    img_path = os.path.join(output_folder, img_name)\n",
    "    \n",
    "    #print(\"Processing:\", img_path)\n",
    "    # Open image and convert to tensor\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    if n==1:\n",
    "        print(\"Pred img shape\",img.size)\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Compute CLIPIQA Score\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        # musiq_score = musiq_metric(img_tensor)\n",
    "        clipiqa_score = clipiqa_metric(img_tensor)\n",
    "\n",
    "    # musiq_scores.append(musiq_score.item())  # Convert tensor to float\n",
    "    clipiqa_scores.append(clipiqa_score.item())\n",
    "\n",
    "# Compute Average Score\n",
    "# avg_musiq = np.mean(musiq_scores)\n",
    "avg_clipiqa = np.mean(clipiqa_scores)\n",
    "\n",
    "# print(f\"\\nAverage MUSIQ Score: {avg_musiq:.4f}\")\n",
    "print(f\"\\nAverage CLIPIQA Score: {avg_clipiqa:.4f}\")\n",
    "# Write to file\n",
    "with open(save_path, 'a') as f:\n",
    "    f.write(f\"Average CLIPIQA Score: {avg_clipiqa:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24625bf2-b659-409e-b6a2-f764d1031b7c",
   "metadata": {},
   "source": [
    "## MusicIQA score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008390d1-38be-4d0d-9e59-be1335e43bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images to process 1380\n",
      "Pred img shape (1024, 1024)\n",
      "\n",
      "Average MUSIQ Score: 55.8166\n",
      "saved to /srv/Data16tb/Kishore/ResShift/results/user_own_data_prosx_t2_inference_SR4_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "musiq_scores = []\n",
    "\n",
    "# Get the list of images in the folder (filter only valid image files)\n",
    "output_images = [f for f in sorted(os.listdir(output_folder)) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "print(\"total images to process\",len(output_images))\n",
    "\n",
    "# # Define image transformation to convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Loop through images\n",
    "for n,img_name in enumerate(output_images):\n",
    "    # print(n,'---->>',img_name)\n",
    "    img_path = os.path.join(output_folder, img_name)\n",
    "    #print(\"Processing:\", img_path)\n",
    "    # Open image and convert to tensor\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    if n==1:\n",
    "        print(\"Pred img shape\",img.size)\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Compute CLIPIQA Score\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        musiq_score = musiq_metric(img_tensor)\n",
    "        # clipiqa_score = clipiqa_metric(img_tensor)\n",
    "    musiq_scores.append(musiq_score.item())  # Convert tensor to float\n",
    "    # clipiqa_scores.append(clipiqa_score.item())\n",
    "# Compute Average Score\n",
    "avg_musiq = np.mean(musiq_scores)\n",
    "# avg_clipiqa = np.mean(clipiqa_scores)\n",
    "print(f\"\\nAverage MUSIQ Score: {avg_musiq:.4f}\")\n",
    "# print(f\"\\nAverage CLIPIQA Score: {avg_clipiqa:.4f}\")\n",
    "# Write to file\n",
    "with open(save_path, 'a') as f:\n",
    "    f.write(f\"Average MUSIQ Score: {avg_musiq:.4f}\\n\")\n",
    "    print(f'saved to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33602984-5b01-4ec0-bc84-bab490b5f5f4",
   "metadata": {},
   "source": [
    "## PSNR ,SSIM, LPIPS EVAUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c58376-8031-4d91-9f50-a1ff2ea4925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(array):\n",
    "    return ((array-np.min(array)))/(np.max(array)-np.min(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ac507b-81ce-4afb-a4d7-0b5375a8a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt /srv/Data16tb/Kishore/ResShift/testdata/user_own_data/Pros_datas/T2_2d_prox_imgs/ \n",
      " Pred /srv/Data16tb/Kishore/ResShift/results/user_own_data_prosx_t2_inference_SR4/\n",
      "1380 1380\n",
      "0000_T2_slice10.png 0000_T2_slice10.png\n"
     ]
    }
   ],
   "source": [
    "base_inp_dir = '/srv/Data16tb/Kishore/ResShift/testdata/user_own_data/Pros_datas'\n",
    "# '/srv/Data16tb/Kishore/ResShift/testdata/Synthetic_datas_from_repo/'\n",
    "base_op_dir = '/srv/Data16tb/Kishore/ResShift/results/'\n",
    "\n",
    "# gt_img_path = glob.glob(base_inp_dir+'/imagenet256_srx4/'+'**/*gt/')[0]\n",
    "gt_img_path = glob.glob(base_inp_dir+'/T2_2d_prox_imgs/')[0]\n",
    "\n",
    "# celeba512-faceir - for blind face restoration\n",
    "# imagenet256_inpainting -  for inpainting task contains 2k images\n",
    "# celeba256_face_inpainting - for face inpainting  - contains 2k face images\n",
    "\n",
    "# pred_img_path = glob.glob(base_op_dir+'/Image_net_test_v1_64_64_32/')[0]\n",
    "pred_img_path = glob.glob(base_op_dir+'user_own_data_prosx_t2_inference_SR4/')[0]\n",
    "\n",
    "# table6_journal_blid_face_restoration_faceIR - prediction of celeba512-faceir blind face restoration\n",
    "# table5_journal_inpaint_face - prediction of imagenet256 inpainting face \n",
    "# table5_inpaint_face_celeba_inference - prediction of face inpainting on celeba\n",
    "# table4_inpaint_on_natural image_imagenet_inference-   inference of imagenet dataset natural inpainting\n",
    "\n",
    "print(\"gt\",gt_img_path,'\\n',\"Pred\",pred_img_path)\n",
    "\n",
    "# Define allowed extensions\n",
    "allowed_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "# Filter only valid image files\n",
    "gt_images = sorted([f for f in sorted(os.listdir(gt_img_path)) if os.path.splitext(f)[1].lower() in allowed_extensions])\n",
    "output_images = sorted([f for f in sorted(os.listdir(pred_img_path)) if os.path.splitext(f)[1].lower() in allowed_extensions])\n",
    "assert len(gt_images)==len(output_images)\n",
    "print(len(gt_images),len(output_images))\n",
    "print(gt_images[0],output_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec115191-1ea9-4e8a-8da8-dba6e53b43b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishore120/kishore_diffusion_env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kishore120/kishore_diffusion_env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/kishore120/kishore_diffusion_env/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kishore120/kishore_diffusion_env/lib/python3.8/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average PSNR: 29.11\n",
      "Average SSIM: 0.9999\n",
      "Average LPIPS: 0.2117 (Lower is better)\n"
     ]
    }
   ],
   "source": [
    "psnr_values = []\n",
    "ssim_values = []\n",
    "lpips_values = []\n",
    "\n",
    "# Initialize LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define image transform for LPIPS\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "for gt_name, out_name in zip(gt_images, output_images):\n",
    "    gt_path = os.path.join(gt_img_path, gt_name)\n",
    "    out_path = os.path.join(pred_img_path, out_name)\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        print(f\"Skipping {out_name} as it does not exist in output folder.\")\n",
    "        continue\n",
    "\n",
    "    # Read images\n",
    "    gt_img = cv2.imread(gt_path)\n",
    "    out_img = cv2.imread(out_path)\n",
    "    # gt_img = norm(cv2.imread(gt_path))\n",
    "    # out_img = norm(cv2.imread(out_path))\n",
    "    \n",
    "    # print(gt_path,'\\n',out_path)\n",
    "    \n",
    "    # print(\"GT img shape\",gt_img.shape, \"Pred img shape\",out_img.shape)\n",
    "\n",
    "    # Resize output image to match ground truth size\n",
    "    out_img_resized = cv2.resize(out_img, (gt_img.shape[1], gt_img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    # print(\"Pred img resized\", out_img_resized.shape)\n",
    "    # print(\"GT image shape\",gt_img.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Convert to grayscale for SSIM calculation\n",
    "    gt_gray = norm(cv2.cvtColor(gt_img, cv2.COLOR_BGR2GRAY))\n",
    "    out_gray = norm(cv2.cvtColor(out_img_resized, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "    # Compute PSNR and SSIM\n",
    "    psnr_value = psnr(gt_img, out_img_resized, data_range=255)\n",
    "    ssim_value = ssim(gt_gray, out_gray, data_range=255)\n",
    "\n",
    "    psnr_values.append(psnr_value)\n",
    "    ssim_values.append(ssim_value)\n",
    "\n",
    "    # Convert images to tensors for LPIPS\n",
    "    gt_tensor = transform(Image.fromarray(cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    out_tensor = transform(Image.fromarray(cv2.cvtColor(out_img_resized, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Compute LPIPS\n",
    "    lpips_value = lpips_model(gt_tensor, out_tensor).item()\n",
    "    lpips_values.append(lpips_value)\n",
    "\n",
    "    #print(f\"{out_name}: PSNR = {psnr_value:.2f}, SSIM = {ssim_value:.4f}, LPIPS = {lpips_value:.4f}\")\n",
    "\n",
    "# Compute average PSNR, SSIM, and LPIPS\n",
    "avg_psnr = np.mean(psnr_values)\n",
    "avg_ssim = np.mean(ssim_values)\n",
    "avg_lpips = np.mean(lpips_values)\n",
    "\n",
    "print(f\"\\nAverage PSNR: {avg_psnr:.2f}\")\n",
    "print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "print(f\"Average LPIPS: {avg_lpips:.4f} (Lower is better)\")\n",
    "\n",
    "# normalize both the folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff99996-9f0d-45a8-9ccb-2efb7e7f65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'a') as f:\n",
    "    f.write(f\"Average PSNR Score: {avg_psnr:.4f}\\n\")\n",
    "    f.write(f\"Average SSIM Score: {avg_ssim:.4f}\\n\")\n",
    "    f.write(f\"Average LPIPS Score: {avg_lpips:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800f0dc-9321-4f8e-9e26-5d09aabdbdac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
